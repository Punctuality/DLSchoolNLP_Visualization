{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today we will speak about text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will detect polarity of movie reviews, negative or positive.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "from os.path import dirname, abspath, join, exists\n",
    "import os\n",
    "import tarfile\n",
    "import argparse\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset.\n",
    "dataset_dir = 'data/'\n",
    "if not exists(dataset_dir):\n",
    "    os.mkdir(dataset_dir)\n",
    "dataset_url = 'https://www.cs.cornell.edu/people/pabo/movie%2Dreview%2Ddata/rt-polaritydata.tar.gz'\n",
    "tar_file = 'rt-polaritydata.tar.gz'\n",
    "tar_filepath = join(dataset_dir, tar_file)\n",
    "urllib.request.urlretrieve(dataset_url, filename=tar_filepath)\n",
    "with tarfile.open(tar_filepath, \"r\") as tar:\n",
    "    tar.extractall(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_filepath = join(dataset_dir,'rt-polaritydata', 'rt-polarity.neg')\n",
    "pos_filepath = join(dataset_dir,'rt-polaritydata', 'rt-polarity.pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May be you will need to change chmod to nltk_data folder\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokens(tokens):\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if token.isalpha() and not token in stop_words:\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open(pos_filepath, 'r', errors='ignore') as pos:\n",
    "    for line in pos:\n",
    "        words = clean_tokens(tokenizer.tokenize(line.lower()))\n",
    "        sentences.append((words, 1))\n",
    "with open(neg_filepath, 'r', errors='ignore') as neg:\n",
    "    for line in neg:\n",
    "        words = clean_tokens(tokenizer.tokenize(line.lower()))\n",
    "        sentences.append((words, 0))\n",
    "        \n",
    "train_data, test_data = train_test_split(sentences, test_size=0.1)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2)\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task: watch balance of classes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all words are equally useful. Some of them are typos or rare words that are only present a few times. \n",
    "â€‹\n",
    "Let's count how many times is each word present in the data so that we can build a \"white list\" of known words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for tokens in train_data:\n",
    "    word_list += tokens[0]\n",
    "for tokens in test_data:\n",
    "    word_list += tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Well, there are quiet few words, so we do not need a complex model. In fact statistical models, for example TF-IDF may work better than NN on this task.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = Counter(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total unique tokens :\", len(token_counts))\n",
    "print('\\n'.join(map(str, token_counts.most_common(n=5))))\n",
    "print('...')\n",
    "print('\\n'.join(map(str, token_counts.most_common()[-3:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many words are there for each count\n",
    "plt.hist(list(token_counts.values()), range=[0, 10**2], bins=50, log=True)\n",
    "plt.xlabel(\"Word counts\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 5\n",
    "\n",
    "# tokens from token_counts keys that had at least min_count occurrences throughout the dataset\n",
    "tokens = [token for token, n in token_counts.items() if n >= min_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a special tokens for unknown and empty words\n",
    "UNK, PAD = \"UNK\", \"PAD\"\n",
    "tokens = [UNK, PAD] + sorted(tokens)\n",
    "print(\"Vocabulary size:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {}\n",
    "for i, token in enumerate(tokens):\n",
    "    token_to_id[token] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])\n",
    "\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "        \n",
    "    max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
    "    \n",
    "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = []\n",
    "train_targets = []\n",
    "for sent, target in train_data:\n",
    "    train_sentences.append(sent)\n",
    "    train_targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's spare RAM.\n",
    "del train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = []\n",
    "test_targets = []\n",
    "for sent, target in test_data:\n",
    "    test_sentences.append(sent)\n",
    "    test_targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(sentences, targets, max_len=None, word_dropout=0):\n",
    "    \"\"\"\n",
    "    Creates a keras-friendly dict from the batch data.\n",
    "    \"\"\"\n",
    "    batch = {}\n",
    "    batch[\"text\"] = as_matrix(sentences, max_len)\n",
    "    batch['target'] = np.array(targets)\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_batch(train_sentences[:10], train_targets[:10], max_len=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's do the architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,n_tokens=len(tokens), emb_size=20, kernel_sizes=[3,4]):\n",
    "        super().__init__()\n",
    "        ### YOUR CODE HERE\n",
    "    \n",
    "    def forward(self,batch):\n",
    "        embeddings = self.embeddings(torch.LongTensor(batch['text']))\n",
    "        embeddings = embeddings.transpose(1,2) # (batch_size, wordvec_size, sentence_length)\n",
    "        \n",
    "        feature_list = []\n",
    "        for conv in self.conv_modules:\n",
    "            ### YOUR CODE HERE\n",
    "        ### YOUR CODE HERE\n",
    "        features = torch.cat(feature_list, dim=1)\n",
    "        features = self.drop(features)\n",
    "        linear = self.linear(features)\n",
    "        return linear\n",
    "    \n",
    "    def predict(self, batch):\n",
    "        return self.softmax(self.forward(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(make_batch(train_sentences[:10], train_targets[:10], max_len=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import random\n",
    "batch_size=25\n",
    "dataset_arange = np.arange(len(train_sentences))\n",
    "num_iters = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = np.array(train_sentences)\n",
    "train_targets = np.array(train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = make_batch(test_sentences, test_targets, max_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train = []\n",
    "losses_test = []\n",
    "for i in tqdm_notebook(range(num_iters)):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    index = np.random.choice(dataset_arange, size=batch_size)\n",
    "    batch = make_batch(train_sentences[index], train_targets[index], max_len=10)\n",
    "    ### YOUR CODE HERE\n",
    "    loss.backward()\n",
    "    if (i+10) % 100 == 0:\n",
    "        losses_train.append(float(loss))\n",
    "        output = model.forward(test_batch)\n",
    "        test_loss = loss_fn(output, Variable(torch.LongTensor(test_batch['target'])))\n",
    "        losses_test.append(float(test_loss))\n",
    "    if (i+10) % 500 == 0:\n",
    "        print(\"Train loss: \", losses_train[-1])\n",
    "        print(\"Test loss: \", losses_test[-1])\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(len(losses_train)), losses_train)\n",
    "plt.scatter(np.arange(len(losses_test)), losses_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(make_batch(test_sentences[:10], test_targets[:10], max_len=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(make_batch(test_sentences, test_targets, max_len=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(predictions.detach().numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test_targets, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Well, it is bad, in the original paper about CNNs https://arxiv.org/pdf/1408.5882.pdf the author achieves nearly 80 percents.**\n",
    "\n",
    "**Hometask: Achive good accuracy with CNN models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework, project part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I want you to choose one topic:\n",
    "#### Each task should be a class in python3, you can choose all over stuff by yourself.\n",
    "\n",
    "#### 1) Write function that will remove words one by one, measure metric changing after that and mark words.\n",
    "#### 2) Finding inputs that maximize/minimize activation of some chosen neurons (_read more [on distill.pub](https://distill.pub/2018/building-blocks/)_)\n",
    "#### 3)Building local linear approximations to your neural network: [article](https://arxiv.org/abs/1602.04938), [eli5 library](https://github.com/TeamHG-Memex/eli5/tree/master/eli5/formatters)\n",
    "\n",
    "#### 4) Any article you would like from https://github.com/blackboxnlp/blackboxnlp.github.io/blob/master/program.md\n",
    "#### 5) You can come up with your ideas too.\n",
    "\n",
    "### Feel free to discuss your ideas or problems with me via email or telegram.\n",
    "\n",
    "#### Almost all of this staff already exists in code, you need only to understand it and change it according your view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
